# Variables
FRAMEWORKS = ['vapor', 'ktor', 'express']
SCENARIOS = ['lipsum', 'json', 'postgres', 'fibonacci']
THREAD_COUNTS = [4, 200, 500]
METRICS = [
    'Throughput',
    'AvgLatency',
    'TailLatency99',
    'ErrorRate',
    'AvgCPU',
    'AvgMemoryMiB'
]
METRIC_LABELS = {
    'Throughput': 'Throughput',
    'AvgLatency': 'Mean latency',
    'TailLatency99': '99th percentile tail latency',
    'ErrorRate': 'Error rate',
    'AvgCPU': 'Mean CPU utilization',
    'AvgMemoryMiB': 'Mean memory utilization'
}
UNIT = {
    'Throughput': 'RPS',
    'AvgLatency': 'ms',
    'TailLatency99': 'ms',
    'ErrorRate': '%',
    'AvgCPU': '%',
    'AvgMemoryMiB': 'MiB'
}

# Benchmark test configuration
NUM_RUNS = 50
DEDICATED_CPU_CORES = 4
RAMPUP_DURATION = 30 # seconds
TEST_DURATION = 120 # seconds

# Paths
RUNS_DIR = '../benchmark/results/' # directory containing run_1, run_2, etc. generated by ../benchmark/run_all_benchmarks.sh
OUTPUT_DIR ='output/'
CHARTS_DIR = OUTPUT_DIR + 'charts/'
ANALYSIS_DIR = OUTPUT_DIR + 'analysis/'
AGGREGATE_PATH = OUTPUT_DIR + 'aggregated_results.csv' # Data source for the analysis scripts (except the aggregation script)
